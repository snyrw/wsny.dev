[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I’m Will Snyder, a current senior at Western Washington University.\nI’m also a self-taught ML researcher currently exploring optimization theory, information-theoretic perspectives on deep learning, as well as other mathematical intuitions and emerging research directions from canonical texts and recent papers. Proficient in Japanese, I’m interested in tracking developments across both English and Japanese research landscapes.\nIf you’re interested in anything I’ve done, don’t be afraid to reach out!"
  },
  {
    "objectID": "posts/intro/intro.html",
    "href": "posts/intro/intro.html",
    "title": "Introduction",
    "section": "",
    "text": "Hello! This is my first post of hopefully many more to come on this site. Here is my background (as of writing) for anybody that’s curious.\n\nWho am I?\nI’m Will Snyder, a senior at WWU studying computer science. Overall, I’ve been on interesting track with my degree with transferring from a separate university that I spent my freshman year at as well as several months spent abroad in and around Tokyo for an exchange program.\nAdding onto the above, I am a long-time learner of Japanese (~8 years now). I spent a few years in high school doing classes before switching to immersion-based learning, to which I still remain serious about even now (although I’m much more casual than I used to be). I currently enjoy Japanese research, older novels (Meiji-era or so), and historical content to keep my intuition fresh.\nI’m currently learning ML using the intuition I gained in self-learning Japanese. This involves a fundamentals-first approach that I will get into detail later on.\n\n\nWhy ML?\nA few months ago, I was reached out to for an ML research position as a result of being part of a specific class at my university. I began studying for several hours a day using the fastai MOOC to make sure I wasn’t going to be a laggard once things started. As I went further, I looked into the landscape and how publishing works expecting that I could’ve advocated for this project to make it somewhere like a NeurIPS workshop. Perhaps that was naive for someone who was as green as I was at the time, but I became seriously invested in the idea of research as this other side of computer science that I never really acknowledged before.\nThe opportunity unfortunately fell through and though I was discouraged for a few days, I remembered that I really enjoyed the rigor of this field so I kept going anyways. I was an absolute beginner again with my back against the wall, which was incredibly refreshing compared to ennui of having exceeded most of my initial goals with Japanese.\nI eventually decided to give myself a challenge: study ML for 1000 hours over 250 days. It overall seemed like a good rule of thumb for reaching a certain level of expertise based on my previous experiences of tracking time like this, so I went ahead with it and am going strong at the moment.\n\n\nWhere I am currently\nI won’t mention the exact hour number considering it’ll be outdated as the days pass, but I will just say that I’m in the first few hundred hours at the moment. I realized that fastai was probably slowing me down compared to where I wanted to get to, so I started reading research with a 2025 NeurIPS paper that seemed interesting. Needless to say, I had little idea of what was happening so I then went back to something more digestible like Vaswani et al. 2017 (the Transformer paper). I passed over that paper a few times as well and still realized that I didn’t know as much as I wanted to know.\nFollowing this, I then constructed (with AI assistance) my own structured ML reading list of a large amount of papers before and after Vaswani et al. 2017. The intuition here was that it marked a significant point in the ML ‘canon’, so to speak. Some broad examples of this are Bengio 2003 (A Neural Probabalistic Language Model), Word2Vec, and Seq2Seq followed by modern techniques like BERT and the GPT line, newer optimizers, scaling laws, and RLHF. This sort of sequential approach actually worked very well at first and solved part of the issue that I had faced before. The jargon and math that I had seen started to become recognizable and that overall signaled to me that I was heading in the right direction.\nI still felt that I wasn’t completely taking in most of what was being said though. I thought about what I should do with this for a while and considered that I could keep learning things through osmosis, but I finally settled on textbooks that teach intuitions from scratch. Boyd & Vandenberghe (convex optimization), Cover & Thomas (information theory), and a few others like Trefethen & Bau (linear algebra), Murphy (specific ML intuitions), and PMPP (CUDA) were all texts I’ve used to get a greater context behind all of the research that I plan on reading. This approach seems even better considering I now know exactly what a specific piece of notation or jargon might be coming from, or at least I can make a much better educated guess as to what it relates to. I will likely continue on this path until I hit diminishing returns after reaching more esoteric content.\nI know that this sort of extreme breadth-first, first principles study routine is somewhat unusual for self-learners, but I found it highly effective for Japanese immersion (ex. read, watch content heavily across a wide ranges of domains before speaking). It isn’t a perfect one-to-one, although I find that being a strong generalist with good fundamentals is preferable across domains despite things taking longer to develop.\n\n\nWhere I’m headed\nMy plans are still flexible while I build expertise, although I plan to continue with posts like these on a frequent basis.\nI don’t necessarily want to jump into things that I can’t speak to without difficulty yet, but I generally plan on shifting things to explaining new papers that may come attached with reproductions, larger scale experiments based on research interests, and mathematical deconstructions of current architectures/issues in the field that I see. I also am interested in a slew of OSS projects so expect breakdowns on things I might do there as well.\nAs it relates to my Japanese studies, I think mixing in some aspects of that to this blog might be valuable. I’m currently thinking of a round-up of sorts for interesting things I’ve seen published at certain venues (mostly JSAI right now), differences between English and Japanese norms, and papers that overall propose things I don’t regularly see on arXiv.\n\n\nConclusion\nThis was a short post explaining who I am, why I do what I do, and what I’m currently doing at the moment. I expect these posts to ramp up in complexity with future ones resembling papers or what was released on Distill (although not nearly as long) in style.\nI’m overall excited to see where I end up later on. I’m committed to sharing as much as I can on what I’m doing given my perspective on already having self-learned a separate equally difficult skill to a high level. Hopefully I can make things slightly less opaque for those interested in learning about this field from scratch."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog Posts",
    "section": "",
    "text": "Introduction\n\n\n\nnon-technical\n\n\n\n\n\n\n\n\n\nAug 30, 2025\n\n\nWill Snyder\n\n\n\n\n\nNo matching items"
  }
]