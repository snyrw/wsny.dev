---
title: "Introduction"
author: "Will Snyder"
date: "2025-08-30"
categories: [non-technical]
image: "image.jpg"
---

Hello! This is my first post of hopefully many more to come on this site. I want to give a bit of a background before I dive into more technical things.

### Who am I?

As mentioned on the front page, I’m Will Snyder, a senior at WWU studying computer science. I’ve been on interesting track with my degree given transferring from a separate university that I spent a year at as well as several months spent abroad in Tokyo.

To add onto that, I am a long-time learner of Japanese. That really was my main focus in my life up until recently despite being a CS major up. I spent several years doing immersion-based learning and still remain serious about it even now, to which I still read Japanese research and older novels (Meiji-era or so) to keep my intuition fresh.

### Why ML?

A few months ago, I was scouted for an ML research position that I started independently preparing for. I began studying for several hours a day using the fastai MOOC to make sure I was as prepared as possible. I started looking into the landscape and how publishing worked as well, expecting that I could’ve advocated for this project to make it somewhere like a NeurIPS workshop. Perhaps that was naive for someone who was as green as I was at the time, but I became seriously invested in the idea of all of this stuff.

It unfortunately fell through and though I was discouraged for a few days, I remembered that I really did enjoy the rigor of this field and feeling like I was an absolute beginner again. It felt a bit like turning over a new leaf after experiencing a bit of ennui from being in a maintenance state with Japanese.

I eventually decided to give myself a challenge: do 1000 hours of studying ML in 250 days. There was some vague reasoning like this it lining up around my graduation date, but pushing myself like this is what I remembered enjoying a bunch about self-learning, and I’ve been keeping up with it ever since.

### Where I am currently

I won’t say the hour number considering it’ll be outdated by the day, but I’ll just say that I’m in the first few hundred hours at the moment. I’ve switched from fastai, to a structured ML reading list of hundreds of papers before and after Vaswani et al. 2017 (think Bengio et al. 2003, Word2Vec through modern techniques, like BERT and all of the GPTs, optimizers, and RLHF), and now finally to combining that with texts like Boyd & Vandenberghe (convex optimization), Cover & Thomas (information theory), and a few others for linear algebra, statistics, and ML intuitions or engineering-focused things so that I learn the deep core of this field before branching out.

I know that this sort of extreme breadth-first, first principles study routine is somewhat unusual for self-learners, but I found it highly effective for Japanese immersion (ex. read, watch content heavily across a wide ranges of domains before speaking). Obviously these aren’t one-to-one, but it’s looking like the overall arc could be generally the same in that learning from the source is better than hand-wavey downstream resources.


### Where I’m headed

My plans are still flexible while I build expertise, but I plan to continue with posts like these on a frequent basis.

I don’t necessarily want to jump into things that I can’t speak to without difficulty yet, but I generally plan on shifting things to explaining new papers that release on arXiv and reproductions, larger scale experiments based on research interests, and mathematical deconstructions of current architectures/issues in the field that I see. I also am interested in a slew of OSS projects, so expect and breakdowns on things I might do there as well.

As it relates to my Japanese studies, I would like to mix in some aspects of that to this blog as well. I’m currently thinking like a round-up of sorts for interesting things I’ve seen at certain venues (JSAI), differences between English and Japanese norms, and papers that overall propose things I don’t regularly see on arXiv.

### Conclusion

This was a short post explaining who I am, why I do what I do, and what I’m currently doing at the moment. As explained above, I expect these posts that ramp up in complexity with future ones resembling papers themselves or what was released on Distill (although not nearly as long).

I’m overall pretty excited to see where I end up later on and remain committed to sharing as much as I can on what I’m doing and how it might be different from whatever else is out there.